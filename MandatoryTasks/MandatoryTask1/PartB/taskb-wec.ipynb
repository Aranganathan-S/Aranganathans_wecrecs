{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q --upgrade transformers datasets evaluate sacrebleu sentencepiece scikit-learn\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:46:21.784530Z","iopub.execute_input":"2025-10-24T12:46:21.784915Z","iopub.status.idle":"2025-10-24T12:46:21.791701Z","shell.execute_reply.started":"2025-10-24T12:46:21.784883Z","shell.execute_reply":"2025-10-24T12:46:21.791089Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer\n)\nimport torch\nimport numpy as np\n\nMODEL_NAME = \"Helsinki-NLP/opus-mt-en-fr\"\nOUTPUT_DIR = \"./translation_model\"\nBATCH_SIZE = 8\nMAX_INPUT_LENGTH = 64\nMAX_TARGET_LENGTH = 64\nPREFIX = \"translate English to French: \"\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Running on:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:46:23.361241Z","iopub.execute_input":"2025-10-24T12:46:23.361850Z","iopub.status.idle":"2025-10-24T12:46:48.599979Z","shell.execute_reply.started":"2025-10-24T12:46:23.361815Z","shell.execute_reply":"2025-10-24T12:46:48.599185Z"}},"outputs":[{"name":"stderr","text":"2025-10-24 12:46:34.500432: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761309994.684491     104 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761309994.734828     104 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Running on: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"I am just using 5% of dataset otherwise the training times are insane","metadata":{}},{"cell_type":"code","source":"print(\"Loading\")\nraw_datasets = load_dataset(\"opus_books\", \"en-fr\")\nfull_train = raw_datasets[\"train\"]\nsample_size = max(1, int(0.05 * len(full_train)))\nval_split = int(0.9 * sample_size)\ntrain = full_train.select(range(val_split))\nval = full_train.select(range(val_split, sample_size))\n\nprint(f\"Train size: {len(train)}, Validation size: {len(val)})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:46:48.601282Z","iopub.execute_input":"2025-10-24T12:46:48.601823Z","iopub.status.idle":"2025-10-24T12:46:52.753072Z","shell.execute_reply.started":"2025-10-24T12:46:48.601803Z","shell.execute_reply":"2025-10-24T12:46:52.752374Z"}},"outputs":[{"name":"stdout","text":"Loading\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b3a058f3df3442f89f790aa4019bf45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"en-fr/train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07dee70a8640423aa5cb92d665a034af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/127085 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e434883de124462a965d1e7011cf3068"}},"metadata":{}},{"name":"stdout","text":"Train size: 5718, Validation size: 636)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\ndef preprocess_examples(examples):\n    inputs = [PREFIX + t[\"en\"] for t in examples[\"translation\"]]\n    targets = [t[\"fr\"] for t in examples[\"translation\"]]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n    labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, truncation=True)\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n    return model_inputs\n\nprint(\"Tokenizing datasets\")\ntrain_tokens = train.map(preprocess_examples, batched=True)\nval_tokens = val.map(preprocess_examples, batched=True)\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:46:52.753973Z","iopub.execute_input":"2025-10-24T12:46:52.754293Z","iopub.status.idle":"2025-10-24T12:47:01.745659Z","shell.execute_reply.started":"2025-10-24T12:46:52.754268Z","shell.execute_reply":"2025-10-24T12:47:01.744901Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac4d7c8b1fef4d82821f514c3672cb80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f84b21c15b4e4cfa9aa211485a12abb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"source.spm:   0%|          | 0.00/778k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39dbbcebde75422ebfea369d0108dd70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"target.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cc2db72fe374552b73b4befee929052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e62fb94ab58474a8e591fd5c96523d7"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n  warnings.warn(\"Recommended: pip install sacremoses.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06228eacb2ce4983a1c90a61d83e3a64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9703391c7c245a6a91f9f40c9e2b76a"}},"metadata":{}},{"name":"stdout","text":"Tokenizing datasets\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/301M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94821f8e6ecc4193b9c31b6df66c0ed0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82c9d220c3904bfd90da44c7c8f5b628"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/636 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3c1031b22524d028315e56d27e520a3"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import EarlyStoppingCallback","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:47:01.747133Z","iopub.execute_input":"2025-10-24T12:47:01.747439Z","iopub.status.idle":"2025-10-24T12:47:01.751124Z","shell.execute_reply.started":"2025-10-24T12:47:01.747420Z","shell.execute_reply":"2025-10-24T12:47:01.750354Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import evaluate\nbleu = evaluate.load(\"sacrebleu\")\n\ndef postprocess_text(preds, labels):\n    preds = [pred.strip() for pred in preds]\n    labels = [[label.strip()] for label in labels]\n    return preds, labels\n\ndef compute_metrics(eval_pred):\n    preds, labels = eval_pred\n    if isinstance(preds, tuple):\n        preds = preds[0]\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n    result = bleu.compute(predictions=decoded_preds, references=decoded_labels)\n    return {\"bleu\": result[\"score\"]}\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    eval_strategy=\"epoch\",\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    predict_with_generate=True,\n    logging_dir=f\"{OUTPUT_DIR}/logs\",\n    save_total_limit=2,\n    num_train_epochs=10,\n    fp16=False,\n    save_strategy=\"epoch\",\n    learning_rate=5e-5,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tokens,\n    eval_dataset=val_tokens,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n    \n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:47:01.752039Z","iopub.execute_input":"2025-10-24T12:47:01.752392Z","iopub.status.idle":"2025-10-24T12:47:03.528442Z","shell.execute_reply.started":"2025-10-24T12:47:01.752363Z","shell.execute_reply":"2025-10-24T12:47:03.527684Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28a9a0a958644dbb9f4ca5ebe65c607c"}},"metadata":{}},{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_104/2207071198.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n  trainer = Seq2SeqTrainer(\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"trainer.train()\ntrainer.save_model(OUTPUT_DIR)\nprint(\"Model trained da!:\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:47:03.529086Z","iopub.execute_input":"2025-10-24T12:47:03.530662Z","iopub.status.idle":"2025-10-24T12:51:55.520170Z","shell.execute_reply.started":"2025-10-24T12:47:03.530640Z","shell.execute_reply":"2025-10-24T12:51:55.519493Z"}},"outputs":[{"name":"stderr","text":"The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None}.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2145' max='7150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2145/7150 04:50 < 11:17, 7.39 it/s, Epoch 3/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.032800</td>\n      <td>2.233086</td>\n      <td>12.906101</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.671600</td>\n      <td>2.251130</td>\n      <td>13.370427</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.198700</td>\n      <td>2.262501</td>\n      <td>13.601515</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:3918: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 512, 'num_beams': 4, 'bad_words_ids': [[59513]]}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.encoder.embed_positions.weight', 'model.decoder.embed_tokens.weight', 'model.decoder.embed_positions.weight', 'lm_head.weight'].\n","output_type":"stream"},{"name":"stdout","text":"Model trained da!:\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\nfrom sklearn.metrics import precision_recall_fscore_support\npreds, refs = [], []\n\nfor example in val:\n    src = PREFIX + example[\"translation\"][\"en\"]\n    input_ids = tokenizer(src, return_tensors=\"pt\").input_ids.to(model.device)\n    output_ids = model.generate(input_ids, max_length=MAX_TARGET_LENGTH, num_beams=4)\n    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n    preds.append(pred)\n    refs.append(example[\"translation\"][\"fr\"])\n\n# BLEU\nbleu = evaluate.load(\"sacrebleu\")\nbleu_result = bleu.compute(predictions=preds, references=[[r] for r in refs])\nprint(f\"\\nBLEU score: {bleu_result['score']:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-24T12:51:55.520952Z","iopub.execute_input":"2025-10-24T12:51:55.521189Z","iopub.status.idle":"2025-10-24T12:54:43.120504Z","shell.execute_reply.started":"2025-10-24T12:51:55.521169Z","shell.execute_reply":"2025-10-24T12:54:43.119815Z"}},"outputs":[{"name":"stdout","text":"\nBLEU score: 11.03\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"obviously the score is kinda low coz i just used 5% of dataset, but i beleive otherwise the models and methods are solid tho","metadata":{}}]}